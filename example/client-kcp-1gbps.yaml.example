# paqet Client Configuration - Optimized for 1Gbps Bandwidth with KCP
# This configuration is tuned for high-bandwidth (1Gbps) low-loss networks
# Role must be explicitly set
role: "client"

# Logging configuration
log:
  level: "info"  # none, debug, info, warn, error, fatal

# SOCKS5 proxy configuration (client mode)
socks5:
  - listen: "127.0.0.1:1080"    # SOCKS5 proxy listen address
    username: ""                # Optional SOCKS5 authentication
    password: ""                # Optional SOCKS5 authentication

# Port forwarding configuration (can be used alongside SOCKS5)
# forward:
#   - listen: "127.0.0.1:8080"  # Local port to listen on
#     target: "127.0.0.1:80"    # Target to forward to (via server)
#     protocol: "tcp"           # Protocol (tcp/udp)

# Network interface settings
network:
  interface: "en0"                          # CHANGE ME: Network interface (en0, eth0, wlan0, etc.)
  # guid: "\Device\NPF_{...}"               # Windows only (Npcap).

  # IPv4 configuration
  ipv4:
    addr: "192.168.1.100:0"                 # CHANGE ME: Local IP (use port 0 for random port)
    router_mac: "aa:bb:cc:dd:ee:ff"         # CHANGE ME: Gateway/router MAC address

  # IPv6 configuration (optional)
  # ipv6:
  #   addr: "[2001:db8::1]:0"               # CHANGE ME: Local IPv6 address and port (optional)
  #   router_mac: "aa:bb:cc:dd:ee:ff"       # CHANGE ME: Gateway/router MAC address for IPv6

  tcp:
    local_flag: ["PA"]                      # Local TCP flags (Push+Ack default)
    remote_flag: ["PA"]                     # Remote TCP flags (Push+Ack default)

  # PCAP settings optimized for high bandwidth
  pcap:
    sockbuf: 33554432                       # 32MB buffer for high packet rates
    send_queue_size: 10000                  # Large queue for burst handling
    max_retries: 3                          # Max retry attempts on write failure
    initial_backoff_ms: 10                  # Initial retry backoff in ms
    max_backoff_ms: 1000                    # Maximum retry backoff in ms

# Server connection settings
server:
  addr: "10.0.0.100:9999"  # CHANGE ME: paqet server address and port

# Performance settings optimized for 1Gbps throughput
performance:
  max_concurrent_streams: 20000             # High concurrency for many connections
  packet_workers: 8                         # More workers for parallel processing
  stream_worker_pool_size: 15000            # Large worker pool
  enable_connection_pooling: false          # Not used on client side
  max_retry_attempts: 5
  retry_initial_backoff_ms: 100
  retry_max_backoff_ms: 10000

# Transport protocol configuration
transport:
  protocol: "kcp"  # Transport protocol: "kcp" or "quic"
  conn: 4          # Multiple connections (1-8) for load distribution at high bandwidth
  
  # Large buffers for high bandwidth
  tcpbuf: 262144   # 256KB TCP buffer for maximum throughput
  udpbuf: 65536    # 64KB UDP buffer for KCP packets

  # KCP protocol settings optimized for 1Gbps bandwidth
  kcp:
    mode: "manual"              # Manual mode for fine-tuned control
    
    # Manual mode parameters optimized for high bandwidth, low latency
    nodelay: 1                  # Enable for lower latency & aggressive retransmission
    interval: 10                # 10ms update interval for responsiveness
    resend: 2                   # Fast retransmit after 2 ACK skips
    nocongestion: 1             # Disable congestion control for maximum speed
    wdelay: false               # Flush immediately for low latency
    acknodelay: true            # Send ACKs immediately

    # Window sizes maximized for 1Gbps bandwidth
    # Bandwidth-Delay Product (BDP) calculation:
    # For 1Gbps with 50ms RTT: BDP = (1000 Mbps * 0.05s) / 8 = 6.25 MB
    # With MTU 1400, need: 6.25MB / 1400 = ~4500 packets in flight
    # We use maximum window size (32768) which allows ~45MB in flight
    mtu: 1400                   # High MTU (close to standard 1500, with overhead)
    rcvwnd: 32768               # Maximum receive window (32768 * 1400 = ~45MB)
    sndwnd: 32768               # Maximum send window (32768 * 1400 = ~45MB)

    # Encryption settings
    block: "aes"                        # AES encryption (good balance of speed/security)
    key: "your-secret-key-here"         # CHANGE ME: Secret key (must match server)

    # Buffer settings optimized for high bandwidth
    smuxbuf: 33554432           # 32MB SMUX buffer for high throughput
    streambuf: 16777216         # 16MB stream buffer for large transfers

# Forward Error Correction (FEC) - DISABLED for 1Gbps on good networks
# FEC adds overhead and is unnecessary on low-loss high-bandwidth links
# Enable only if your network has significant packet loss (>1%)
#   dshard: 10    # Data shards for FEC
#   pshard: 3     # Parity shards for FEC

# ============================================================================
# CONFIGURATION NOTES FOR 1Gbps THROUGHPUT
# ============================================================================
#
# This configuration is optimized for:
# - High bandwidth: 1Gbps+ capable links
# - Low latency: <50ms RTT networks
# - Low packet loss: <0.5% loss rate
#
# Key optimizations:
# 1. Maximum KCP window sizes (32768) for high bandwidth-delay product
# 2. Large MTU (1400) to reduce per-packet overhead
# 3. Multiple connections (conn: 4) for load distribution
# 4. Aggressive retransmission (nodelay: 1, interval: 10, resend: 2)
# 5. No congestion control (nocongestion: 1) for maximum speed
# 6. Large buffers (256KB TCP, 64KB UDP, 32MB SMUX)
# 7. Parallel packet workers (8 workers) for multi-core systems
# 8. High PCAP buffer (32MB) to handle bursts
#
# For different network conditions:
# - High latency (>100ms): Increase rcvwnd/sndwnd even more if possible
# - High loss (>1%): Enable FEC (dshard/pshard) and consider fast2/fast3 mode
# - Lower bandwidth: Reduce window sizes and buffers proportionally
#
# System tuning recommendations:
# - Increase OS UDP buffer sizes: sysctl -w net.core.rmem_max=26214400
# - Increase OS UDP buffer sizes: sysctl -w net.core.wmem_max=26214400
# - Monitor CPU usage: May need to adjust packet_workers
# ============================================================================
